# AIMO3 Eval æ¡†æ¶ ğŸš€

> é€‚é…æ–°ç‰ˆä»£ç çš„ç»Ÿä¸€è¯„æµ‹ä¸è¿è¡Œè¯´æ˜ï¼ˆä¸­æ–‡/Englishï¼‰

A lightweight evaluation framework for the Kaggle AIMO3 competition, supporting both **local** and **remote** model inference modes.

---

## ğŸ“‹ Table of Contents

- [English](#english)
- [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

### ğŸ¯ Features

- âœ… **Dual Mode Support**: Run with local vLLM server or remote APIs (OpenAI, DeepSeek, etc.)
- âœ… **Flexible Data Loading**: Support CSV files or custom problem lists
- âœ… **Multiple Evaluation Metrics**: Pass@1, Majority@k, and more
- âœ… **Parallel Processing**: Concurrent problem solving with configurable workers
- âœ… **Mathematical Grading**: Intelligent answer extraction and equivalence checking using SymPy
- âœ… **Result Recording**: Automatic parquet export with detailed attempt logs
- âœ… **Easy Integration**: Simple, clean API with sensible defaults

### ğŸ—ï¸ Architecture

```
aimo3_eval/
â”œâ”€â”€ config.py           # ğŸ”§ Configuration management
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ runner.py      # ğŸƒ Main evaluation orchestrator
â”‚   â”œâ”€â”€ solver.py      # ğŸ§  Problem solver implementation
â”‚   â”œâ”€â”€ sandbox.py     # ğŸ“¦ Safe code execution environment
â”‚   â””â”€â”€ vllm_server.py # ğŸ–¥ï¸ Local vLLM server manager
â”œâ”€â”€ data/
â”‚   â””â”€â”€ loader.py      # ğŸ“¥ Data loading utilities
â””â”€â”€ metrics/
    â”œâ”€â”€ evaluator.py   # ğŸ“Š Metrics calculation
    â”œâ”€â”€ extractor.py   # ğŸ” Answer extraction from responses
    â””â”€â”€ math_utils.py  # ğŸ§® Mathematical operations
```

### ğŸš€ Quick Start

#### 1ï¸âƒ£ Installation

```bash
# Clone the repository
git clone <repo-url>
cd aimo3_eval

# Install dependencies (requires Python 3.11+)
uv pip install -e .
# or
pip install -e .
```

#### 2ï¸âƒ£ Configuration

Set up your environment:

```bash
# For remote mode (OpenAI/DeepSeek API)
export OPENAI_API_KEY="your-api-key"
export OPENAI_API_BASE="https://api.deepseek.com"  # or your provider's URL

# For local mode, set model path (optional)
export MODEL_PATH="/path/to/model"
```

#### 3ï¸âƒ£ Basic Usage

```python
from aimo3_eval import CFG, CoTSolver, TIRSolver, DataLoader, EvalRunner

# Configure
cfg = CFG(
    mode='remote',
    remote_model_name="deepseek-reasoner",
    remote_base_url="https://api.deepseek.com",
    remote_api_key="your-key",
    attempts=8,  # Number of tries per problem
    workers=4    # Parallel workers
)

# Load data (map Kaggle's 'answer' to 'ground_truth')
df = DataLoader.load_csv(
    "data/reference.csv",
    id_col="id",
    problem_col="problem",
    ground_truth_col="answer",
)
# or create custom dataset:
# df = DataLoader.load_custom_data(
#     problems=["What is 2+2?"],
#     ids=["demo_1"],
#     ground_truths=["4"]
# )

# Solve and evaluate (choose one solver)
solver = TIRSolver(cfg)
# solver = CoTSolver(cfg)
runner = EvalRunner(cfg, solver)
results = runner.load_data(df).run()
```

### ğŸ”§ Configuration Options

The `CFG` class provides fine-grained control:

```python
cfg = CFG(
    # Mode: 'local' or 'remote'
    mode='remote',
  
    # Remote API settings
    remote_base_url="https://api.deepseek.com",
    remote_api_key="your-key",
    remote_model_name="deepseek-reasoner",
  
    # Local model settings (for mode='local')
    model_path="/path/to/model",
    gpu_memory_utilization=0.90,
    tensor_parallel_size=1,
  
    # Inference parameters
    temperature=0.7,
    top_p=0.95,
    max_tokens=16384,
  
    # Evaluation settings
    attempts=8,           # Number of attempts per problem (for Maj@k)
    workers=8,           # Concurrent workers
    timeout_per_problem=300,
  
    # Output
    output_dir="./outputs"
)
```

### ğŸ“Š Output Format

Results are saved in `outputs/{timestamp}/`:

- **attempts.parquet**: Detailed log of all attempts with answers
- **times.parquet**: Timing statistics
- **metrics.json**: Final metrics (Pass@1, Maj@k, etc.)

### ğŸ“ Metrics Explained

- **Pass@1**: Percentage of correct answers on first attempt
- **Maj@k**: Majority voting accuracy across k attempts
- **Attempt Statistics**: Min/max/average solving time per problem

### ğŸ’¡ Advanced Usage

#### Local Mode with Custom Model

```python
cfg = CFG(
    mode='local',
    model_path="/path/to/your/model",
    served_model_name="my-model",
    tensor_parallel_size=2,  # For multi-GPU
)
server = VLLMServer(cfg)
server.start()
```

#### Custom Prompts

```python
cfg.system_prompt = "Your custom system prompt..."
cfg.tool_prompt = "Your tool usage instructions..."
```

### âš™ï¸ Dependencies

- `python>=3.11`
- `openai>=2.14.0` - API client
- `polars>=1.36.1` - Data processing
- `sympy>=1.14.0` - Mathematical operations
- `jupyter>=1.1.1` - Interactive notebooks

### ğŸ“ Notes

- ğŸ”’ The framework uses a sandboxed environment for code execution
- ğŸ§® Mathematical equivalence is checked using SymPy (e.g., `sqrt(4)` == `2`)
- â±ï¸ Each problem has a configurable timeout to prevent hanging
- ğŸ¯ System prompt is optimized for IMO-style problems

### ğŸ¤ Contributing

Contributions welcome! Please ensure code follows the existing structure and includes proper documentation.

---

## ä¸­æ–‡

### ğŸ¯ åŠŸèƒ½ç‰¹æ€§

- âœ… **åŒæ¨¡å¼æ”¯æŒ**ï¼šæ”¯æŒæœ¬åœ° vLLM æœåŠ¡æˆ–è¿œç¨‹ APIï¼ˆOpenAIã€DeepSeek ç­‰ï¼‰
- âœ… **çµæ´»çš„æ•°æ®åŠ è½½**ï¼šæ”¯æŒ CSV æ–‡ä»¶æˆ–è‡ªå®šä¹‰é—®é¢˜åˆ—è¡¨
- âœ… **å¤šç§è¯„ä¼°æŒ‡æ ‡**ï¼šPass@1ã€Majority@k ç­‰
- âœ… **å¹¶è¡Œå¤„ç†**ï¼šå¯é…ç½®çš„å¹¶å‘ Worker æ•°
- âœ… **æ•°å­¦è¯„åˆ†**ï¼šä½¿ç”¨ SymPy è¿›è¡Œæ™ºèƒ½ç­”æ¡ˆæå–å’Œç­‰ä»·æ€§æ£€æŸ¥
- âœ… **ç»“æœè®°å½•**ï¼šè‡ªåŠ¨å¯¼å‡º parquet æ ¼å¼çš„è¯¦ç»†æ—¥å¿—
- âœ… **æ˜“äºé›†æˆ**ï¼šç®€æ´çš„ API å’Œåˆç†çš„é»˜è®¤é…ç½®

### ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
aimo3_eval/
â”œâ”€â”€ config.py           # ğŸ”§ é…ç½®ç®¡ç†
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ runner.py      # ğŸƒ ä¸»è¯„ä¼°åè°ƒå™¨
â”‚   â”œâ”€â”€ solver.py      # ğŸ§  é—®é¢˜æ±‚è§£å™¨
â”‚   â”œâ”€â”€ sandbox.py     # ğŸ“¦ å®‰å…¨ä»£ç æ‰§è¡Œç¯å¢ƒ
â”‚   â””â”€â”€ vllm_server.py # ğŸ–¥ï¸ æœ¬åœ° vLLM æœåŠ¡å™¨ç®¡ç†
â”œâ”€â”€ data/
â”‚   â””â”€â”€ loader.py      # ğŸ“¥ æ•°æ®åŠ è½½å·¥å…·
â””â”€â”€ metrics/
    â”œâ”€â”€ evaluator.py   # ğŸ“Š æŒ‡æ ‡è®¡ç®—
    â”œâ”€â”€ extractor.py   # ğŸ” ç­”æ¡ˆæå–
    â””â”€â”€ math_utils.py  # ğŸ§® æ•°å­¦è¿ç®—
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1ï¸âƒ£ å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone <repo-url>
cd aimo3_eval

# å®‰è£…ä¾èµ–ï¼ˆéœ€è¦ Python 3.11+ï¼‰
uv pip install -e .
# æˆ–
pip install -e .
```

#### 2ï¸âƒ£ é…ç½®ç¯å¢ƒ

```powershell
# è¿œç¨‹æ¨¡å¼ï¼ˆOpenAI/DeepSeek APIï¼‰
$env:OPENAI_API_KEY = "your-api-key"
$env:OPENAI_API_BASE = "https://api.deepseek.com"

# æœ¬åœ°æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
$env:MODEL_PATH = "C:\\path\\to\\model"
```

#### 3ï¸âƒ£ åŸºç¡€ç”¨æ³•

```python
from aimo3_eval import CFG, CoTSolver, TIRSolver, DataLoader, EvalRunner

# é…ç½®
cfg = CFG(
    mode='remote',
    remote_model_name="deepseek-reasoner",
    remote_base_url="https://api.deepseek.com",
    remote_api_key="your-key",
    attempts=8,    # æ¯ä¸ªé—®é¢˜çš„å°è¯•æ¬¡æ•°
    workers=4      # å¹¶è¡Œ Worker æ•°
)

# åŠ è½½æ•°æ®ï¼ˆè‹¥åŸåˆ—ä¸º answerï¼Œä¼šè‡ªåŠ¨æ˜ å°„ä¸º ground_truthï¼‰
df = DataLoader.load_csv("data/reference.csv", id_col="id", problem_col="problem", ground_truth_col="answer")
# æˆ–è‡ªå®šä¹‰æ•°æ®é›†ï¼š
# df = DataLoader.load_custom_data(
#     problems=["2+2ç­‰äºå¤šå°‘ï¼Ÿ"],
#     ids=["demo_1"],
#     ground_truths=["4"]
# )

# æ±‚è§£å¹¶è¯„ä¼°ï¼ˆäºŒé€‰ä¸€ï¼‰
solver = TIRSolver(cfg)
# solver = CoTSolver(cfg)
runner = EvalRunner(cfg, solver)
results = runner.load_data(df).run()
```

### ğŸ”§ é…ç½®è¯´æ˜

`CFG` ç±»æä¾›ç²¾ç»†çš„æ§åˆ¶é€‰é¡¹ï¼š

```python
cfg = CFG(
    # æ¨¡å¼ï¼š'local' æˆ– 'remote'
    mode='remote',
  
    # è¿œç¨‹ API è®¾ç½®
    remote_base_url="https://api.deepseek.com",
    remote_api_key="your-key",
    remote_model_name="deepseek-reasoner",
  
    # æœ¬åœ°æ¨¡å‹è®¾ç½®ï¼ˆmode='local' æ—¶ä½¿ç”¨ï¼‰
    model_path="/path/to/model",
    gpu_memory_utilization=0.90,
    tensor_parallel_size=1,
  
    # æ¨ç†å‚æ•°
    temperature=0.7,
    top_p=0.95,
    max_tokens=16384,
  
    # è¯„ä¼°è®¾ç½®
    attempts=8,              # æ¯ä¸ªé—®é¢˜çš„å°è¯•æ¬¡æ•°ï¼ˆç”¨äº Maj@kï¼‰
    workers=8,              # å¹¶å‘ Worker æ•°
    timeout_per_problem=300,
  
    # è¾“å‡ºè·¯å¾„
    output_dir="./outputs"
)
```

### ğŸ“Š è¾“å‡ºæ ¼å¼

ç»“æœä¿å­˜åœ¨ `outputs/{æ—¶é—´æˆ³}/` ç›®å½•ä¸‹ï¼š

- **attempts.parquet**: æ‰€æœ‰å°è¯•çš„è¯¦ç»†æ—¥å¿—
- **times.parquet**: æ—¶é—´ç»Ÿè®¡æ•°æ®
- **metrics.json**: æœ€ç»ˆæŒ‡æ ‡ï¼ˆPass@1ã€Maj@k ç­‰ï¼‰

### ğŸ“ æŒ‡æ ‡è¯´æ˜

- **Pass@1**: ç¬¬ä¸€æ¬¡å°è¯•æˆåŠŸçš„ç™¾åˆ†æ¯”
- **Maj@k**: åœ¨ k æ¬¡å°è¯•ä¸­æŠ•ç¥¨æ­£ç¡®çš„ç™¾åˆ†æ¯”
- **Attempt Statistics**: æ¯ä¸ªé—®é¢˜çš„æ±‚è§£æ—¶é—´ç»Ÿè®¡

### ğŸ’¡ é«˜çº§ç”¨æ³•

#### æœ¬åœ°æ¨¡å¼ä¸è‡ªå®šä¹‰æ¨¡å‹

```python
cfg = CFG(
    mode='local',
    model_path="/path/to/your/model",
    served_model_name="my-model",
    tensor_parallel_size=2,  # å¤š GPU å¹¶è¡Œ
)
server = VLLMServer(cfg)
server.start()
```

#### è‡ªå®šä¹‰æç¤ºè¯

```python
cfg.system_prompt = "ä½ çš„è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯..."
cfg.tool_prompt = "ä½ çš„å·¥å…·ä½¿ç”¨è¯´æ˜..."
```

### âš™ï¸ ä¾èµ–é¡¹

- `python>=3.11`
- `openai>=2.14.0` - API å®¢æˆ·ç«¯
- `polars>=1.36.1` - æ•°æ®å¤„ç†
- `sympy>=1.14.0` - æ•°å­¦è¿ç®—
- `jupyter>=1.1.1` - äº¤äº’å¼ç¬”è®°æœ¬

### ğŸ“ æ³¨æ„äº‹é¡¹

- ğŸ”’ æ¡†æ¶ä½¿ç”¨æ²™ç®±ç¯å¢ƒæ‰§è¡Œä»£ç ï¼Œç¡®ä¿å®‰å…¨æ€§
- ğŸ§® æ•°å­¦ç­‰ä»·æ€§é€šè¿‡ SymPy æ£€æŸ¥ï¼ˆä¾‹å¦‚ `sqrt(4)` == `2`ï¼‰
- â±ï¸ æ¯ä¸ªé—®é¢˜éƒ½æœ‰å¯é…ç½®çš„è¶…æ—¶æ—¶é—´ä»¥é˜²æ­¢å¡é¡¿
- ğŸ¯ ç³»ç»Ÿæç¤ºè¯å·²é’ˆå¯¹ IMO é£æ ¼çš„é—®é¢˜è¿›è¡Œä¼˜åŒ–

### ğŸ¤ å‚ä¸è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·ç¡®ä¿ä»£ç éµå¾ªç°æœ‰ç»“æ„å¹¶åŒ…å«é€‚å½“çš„æ–‡æ¡£è¯´æ˜ã€‚

---

**Generated by GitHub Copilot**

