import subprocess
import sys
import time
import os
import requests
from aimo3_eval.config import CFG

class VLLMServer:
    def __init__(self, cfg: CFG):
        self.cfg = cfg
        self.process = None
        self.log_file = None

    def start(self):
        """å¯åŠ¨ vLLM æœåŠ¡"""
        print(f"ğŸš€ Starting vLLM server on port {self.cfg.local.port}...")
        
        cmd = [
            sys.executable, '-m', 'vllm.entrypoints.openai.api_server',
            '--model', self.cfg.local.model_path,
            '--served-model-name', self.cfg.local.served_model_name,
            '--tensor-parallel-size', str(self.cfg.local.tensor_parallel_size),
            '--gpu-memory-utilization', str(self.cfg.local.gpu_memory_utilization),
            '--port', str(self.cfg.local.port),
            '--trust-remote-code',
            '--disable-log-requests' # å‡å°‘æ—¥å¿—å¹²æ‰°
        ]
        
        if self.cfg.local.max_model_len is not None:
            cmd.extend(['--max-model-len', str(self.cfg.local.max_model_len)])
        
        # æ·»åŠ é¢å¤–çš„ vLLM server å¯åŠ¨å‚æ•°
        if self.cfg.local.vllm_server_kwargs:
            for key, value in self.cfg.local.vllm_server_kwargs.items():
                # å°†ä¸‹åˆ’çº¿è½¬ä¸ºè¿å­—ç¬¦ï¼ˆPythoné£æ ¼ -> CLIé£æ ¼ï¼‰
                cli_key = f'--{key.replace("_", "-")}'
                if isinstance(value, bool):
                    # å¸ƒå°”å€¼ï¼šTrue æ—¶æ·»åŠ æ ‡å¿—ï¼ŒFalse æ—¶è·³è¿‡
                    if value:
                        cmd.append(cli_key)
                else:
                    # å…¶ä»–ç±»å‹ï¼šæ·»åŠ é”®å€¼å¯¹
                    cmd.extend([cli_key, str(value)])
        
        os.makedirs(self.cfg.output_dir, exist_ok=True)
        self.log_file = open(os.path.join(self.cfg.output_dir, 'vllm_server.log'), 'w')
        
        self.process = subprocess.Popen(
            cmd,
            stdout=self.log_file,
            stderr=subprocess.STDOUT,
            start_new_session=True # ç¡®ä¿ä½œä¸ºç‹¬ç«‹è¿›ç¨‹ç»„è¿è¡Œ
        )
        
        self._wait_for_ready()

    def _wait_for_ready(self):
        base_url = f"http://localhost:{self.cfg.local.port}/v1/models"
        start_time = time.time()
        
        while True:
            if time.time() - start_time > self.cfg.local.server_timeout:
                self.stop()
                raise TimeoutError("vLLM Server start timeout")
                
            if self.process.poll() is not None:
                raise RuntimeError("vLLM Server process exited unexpectedly")
                
            try:
                resp = requests.get(base_url)
                if resp.status_code == 200:
                    print(f"âœ… vLLM Server ready in {time.time() - start_time:.1f}s")
                    return
            except requests.exceptions.ConnectionError:
                pass
            
            time.sleep(2)

    def stop(self):
        """å®‰å…¨å…³é—­æœåŠ¡"""
        if self.process:
            print("ğŸ›‘ Stopping vLLM server...")
            self.process.terminate()
            try:
                self.process.wait(timeout=5)
            except subprocess.TimeoutError:
                self.process.kill()
        
        if self.log_file:
            self.log_file.close()