import json
import time
import re
import queue
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, TYPE_CHECKING
from collections import Counter

from openai import OpenAI
from aimo3_eval.engine.sandbox import AIMO3Sandbox

# Harmony æ¨¡æ¿æ”¯æŒï¼ˆå¯é€‰å¯¼å…¥ï¼‰
try:
    from openai_harmony import (
        HarmonyEncodingName,
        load_harmony_encoding,
        SystemContent,
        ReasoningEffort,
        ToolNamespaceConfig,
        Author,
        Message,
        Role,
        TextContent,
        Conversation
    )
    HARMONY_AVAILABLE = True
except ImportError:
    HARMONY_AVAILABLE = False

# ä¸ºäº†ç±»å‹æç¤ºï¼Œé˜²æ­¢è¿è¡Œæ—¶å¾ªç¯å¼•ç”¨
if TYPE_CHECKING:
    from aimo3_eval.config import CFG

# --- å®šä¹‰å·¥å…· Schema ---
PYTHON_TOOL = [
    {
        "type": "function",
        "function": {
            "name": "python_interpreter",
            "description": "Execute Python code in a stateful Jupyter notebook environment. Use print() to see outputs.",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The python code to execute. Variables are preserved between calls."
                    }
                },
                "required": ["code"]
            }
        }
    }
]

class TIRSolver:
    def __init__(self, cfg: "CFG"):
        self.cfg = cfg
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è¿æ¥åœ°å€
        if self.cfg.mode == 'remote':
            print(f"ğŸŒ Connecting to Remote API: {self.cfg.remote.model}")
            api_key = self.cfg.remote.api_key
            base_url = self.cfg.remote.base_url
            self.target_model = self.cfg.remote.model
        else:
            print(f"ğŸ  Connecting to Local vLLM: {self.cfg.local.served_model_name}")
            api_key = "sk-local"
            base_url = f"http://localhost:{self.cfg.local.port}/v1"
            self.target_model = self.cfg.local.served_model_name

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=cfg.solver.timeout_per_problem
        )
        
        # åˆå§‹åŒ– Sandbox Pool
        self.sandbox_pool = queue.Queue()
        self._init_sandboxes()

    def _init_sandboxes(self):
        print(f"ğŸ”§ Initializing {self.cfg.solver.workers} sandboxes...")
        with ThreadPoolExecutor(max_workers=self.cfg.solver.workers) as exe:
            # ä¼ é€’ timeout å‚æ•°
            futures = [exe.submit(AIMO3Sandbox, timeout=30) for _ in range(self.cfg.solver.workers)]
            for f in as_completed(futures):
                self.sandbox_pool.put(f.result())
        print("âœ… Sandboxes ready.")

    def solve(self, problem: str, problem_id: str) -> Dict[str, Any]:
        """
        Orchestrator: å¹¶å‘æ‰§è¡Œå¤šæ¬¡å°è¯• (Maj@k)
        """
        start_time = time.time()
        attempts_data = []
        
        # å¹¶è¡Œæ‰§è¡Œ k æ¬¡é‡‡æ ·
        with ThreadPoolExecutor(max_workers=self.cfg.solver.workers) as executor:
            futures = []
            for i in range(self.cfg.solver.attempts):
                futures.append(executor.submit(self._run_single_attempt, problem, i))
                
            for future in as_completed(futures):
                attempts_data.append(future.result())

        # ç®€å•çš„ç­”æ¡ˆèšåˆ (Extract Final Answer)
        valid_answers = [a['final_answer'] for a in attempts_data if a['final_answer'] is not None]
        
        # ä¼—æ•°æŠ•ç¥¨ (Majority Vote)
        if valid_answers:
            from collections import Counter
            final_consensus = Counter(valid_answers).most_common(1)[0][0]
        else:
            final_consensus = None

        # è®¡ç®—æ‰€æœ‰ attempts çš„æ—¶é—´ç»Ÿè®¡
        attempt_times = [a['time_taken'] for a in attempts_data]
        min_time = min(attempt_times) if attempt_times else 0
        max_time = max(attempt_times) if attempt_times else 0
        avg_time = sum(attempt_times) / len(attempt_times) if attempt_times else 0

        return {
            "id": problem_id,
            "problem": problem,
            "final_answer": final_consensus,
            "attempts": attempts_data,  # ä¿å­˜å®Œæ•´è½¨è¿¹ä¾›å¤ç›˜
            "min_attempt_time": min_time,  # æœ€çŸ­ attempt æ—¶é—´
            "max_attempt_time": max_time,  # æœ€é•¿ attempt æ—¶é—´
            "avg_attempt_time": avg_time   # å¹³å‡ attempt æ—¶é—´
        }

    def _run_single_attempt(self, problem: str, attempt_idx: int) -> Dict[str, Any]:
        """
        Core Logic: å•æ¬¡ TIR (Tool-Integrated Reasoning) å¾ªç¯
        """
        attempt_start_time = time.time()  # è®°å½• attempt å¼€å§‹æ—¶é—´
        sandbox = self.sandbox_pool.get() # ä»æ± ä¸­è·å–æ²™ç®±
        
        messages = [
            {"role": "system", "content": self.cfg.prompts.system_prompt},
            {"role": "user", "content": problem}
        ]
        
        final_answer = None
        turn_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        python_calls = 0
        python_errors = 0
        total_tokens = 0
        
        try:
            # --- The Main Loop ---
            # ä½¿ç”¨ getattr è·å– max_turnsï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™é»˜è®¤ 7
            max_turns = self.cfg.inference.max_turns
            
            while turn_count < max_turns:
                turn_count += 1
                
                # 1. è°ƒç”¨ LLM
                try:
                    completion_kwargs = {
                        'model': self.target_model,
                        'messages': messages,
                        'tools': PYTHON_TOOL,
                        'temperature': self.cfg.inference.temperature,
                        'top_p': self.cfg.inference.top_p,
                    }
                    if self.cfg.inference.max_tokens is not None:
                        completion_kwargs['max_tokens'] = self.cfg.inference.max_tokens
                    
                    # æ·»åŠ é¢å¤–çš„æ¨ç†å‚æ•°
                    if self.cfg.inference.extra:
                        completion_kwargs.update(self.cfg.inference.extra)
                    
                    response = self.client.chat.completions.create(**completion_kwargs)
                    message = response.choices[0].message
                    
                    # ç´¯åŠ  tokens
                    if hasattr(response, 'usage') and response.usage:
                        total_tokens += response.usage.total_tokens
                        
                except Exception as e:
                    messages.append({"role": "system", "content": f"Error: {str(e)}"})
                    break

                # 2. å°†æ¨¡å‹çš„å›å¤åŠ å…¥å†å²ï¼Œè½¬ä¸ºçº¯å­—å…¸ï¼Œé¿å…ä¸‹æ¬¡è¯·æ±‚æºå¸¦ SDK å¯¹è±¡
                messages.append(self._normalize_message(message))

                # 3. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ (Tool Calls)
                if message.tool_calls:
                    for tool_call in message.tool_calls:
                        if tool_call.function.name == "python_interpreter":
                            python_calls += 1  # è®°å½• Python è°ƒç”¨æ¬¡æ•°
                            
                            # A. è§£æä»£ç 
                            try:
                                arguments = json.loads(tool_call.function.arguments)
                                code = arguments.get("code", "")
                            except json.JSONDecodeError:
                                code = ""
                                output = "Error: Invalid JSON format in tool arguments."
                                python_errors += 1

                            # B. æ²™ç®±æ‰§è¡Œ
                            if code:
                                try:
                                    output = sandbox.execute(code)
                                    if len(output) > 2000:
                                        output = output[:2000] + "\n...[Output Truncated]"
                                except Exception as e:
                                    output = f"Execution Error: {str(e)}"
                                    python_errors += 1  # è®°å½•æ‰§è¡Œé”™è¯¯
                            else:
                                output = "Error: No code provided."
                                python_errors += 1

                            # C. å°†ç»“æœè¿½åŠ å›æ¶ˆæ¯åˆ—è¡¨ (Role: Tool)
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "name": tool_call.function.name,
                                "content": output
                            })
                
                # 4. å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»å¾—å‡ºç­”æ¡ˆ
                else:
                    content = message.content or ""
                    if "\\boxed{" in content:
                        extracted = self._extract_boxed_content(content)
                        if extracted:
                            final_answer = extracted
                            break
                    
        except Exception as e:
            print(f"Critical Error in attempt {attempt_idx}: {e}")
        finally:
            sandbox.reset()
            self.sandbox_pool.put(sandbox)

        # === å…³é”®ä¿®å¤ï¼šæ¸…æ´— messages å¯¹è±¡ï¼Œé˜²æ­¢ Polars æŠ¥é”™ ===
        clean_messages = []
        for msg in messages:
            if hasattr(msg, "model_dump"):
                # OpenAI v1.x+ (Pydantic V2)
                clean_messages.append(msg.model_dump())
            elif hasattr(msg, "to_dict"):
                # æ—§ç‰ˆæœ¬æˆ–æŸäº›å…¼å®¹åº“
                clean_messages.append(msg.to_dict())
            elif isinstance(msg, dict):
                clean_messages.append(msg)
            else:
                # å…œåº•è½¬æ¢
                try:
                    clean_messages.append(dict(msg))
                except:
                    # æœ€åçš„é˜²çº¿ï¼šè½¬å­—ç¬¦ä¸²
                    clean_messages.append({"role": "unknown", "content": str(msg)})

        return {
            "attempt_id": attempt_idx,
            "final_answer": final_answer,
            "messages": clean_messages,  # <--- è¿”å›æ¸…æ´—åçš„å­—å…¸åˆ—è¡¨
            "time_taken": time.time() - attempt_start_time,  # è®°å½•è¯¥ attempt çš„è€—æ—¶
            "python_calls": python_calls,  # Python è°ƒç”¨æ¬¡æ•°
            "python_errors": python_errors,  # Python é”™è¯¯æ¬¡æ•°
            "total_tokens": total_tokens  # æ€» token ä½¿ç”¨é‡
        }

    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """
        ç®€å•çš„æ­£åˆ™æå–ï¼ŒPhase 2 å·²æœ‰æ›´å¼ºçš„ Extractorï¼Œè¿™é‡Œåªæ˜¯ Loop å†…çš„å¿«é€Ÿæ£€æŸ¥
        """
        matches = re.findall(r'\\boxed\s*\{(.*?)\}', text)
        if matches:
            return matches[-1]
        return None

    def _normalize_message(self, msg: Any) -> Dict[str, Any]:
        """ç¡®ä¿æ¶ˆæ¯ä¸ºçº¯ dictï¼Œé¿å… ChatCompletionMessage ç­‰ SDK å¯¹è±¡åœ¨ä¸‹ä¸€è½®è°ƒç”¨å‡ºé”™"""
        if hasattr(msg, "model_dump"):
            return msg.model_dump()
        if hasattr(msg, "to_dict"):
            return msg.to_dict()
        if isinstance(msg, dict):
            return msg
        try:
            return dict(msg)
        except Exception:
            return {"role": "unknown", "content": str(msg)}

    def cleanup(self):
        """Shut down sandboxes"""
        while not self.sandbox_pool.empty():
            try:
                sb = self.sandbox_pool.get_nowait()
                sb.close()
            except:
                pass


class CoTSolver:
    """
    çº¯ Chain-of-Thought Solverï¼Œä¸ä½¿ç”¨ä»»ä½•å·¥å…·ã€‚
    æ¨¡å‹ç›´æ¥é€šè¿‡æ¨ç†å¾—å‡ºç­”æ¡ˆã€‚
    """
    
    def __init__(self, cfg: "CFG"):
        self.cfg = cfg
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è¿æ¥åœ°å€
        if self.cfg.mode == 'remote':
            print(f"ğŸŒ [CoT] Connecting to Remote API: {self.cfg.remote.model}")
            api_key = self.cfg.remote.api_key
            base_url = self.cfg.remote.base_url
            self.target_model = self.cfg.remote.model
        else:
            print(f"ğŸ  [CoT] Connecting to Local vLLM: {self.cfg.local.served_model_name}")
            api_key = "sk-local"
            base_url = f"http://localhost:{self.cfg.local.port}/v1"
            self.target_model = self.cfg.local.served_model_name

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=cfg.solver.timeout_per_problem
        )

    def solve(self, problem: str, problem_id: str) -> Dict[str, Any]:
        """
        Orchestrator: å¹¶å‘æ‰§è¡Œå¤šæ¬¡å°è¯• (Maj@k)
        """
        start_time = time.time()
        attempts_data = []
        
        # å¹¶è¡Œæ‰§è¡Œ k æ¬¡é‡‡æ ·
        with ThreadPoolExecutor(max_workers=self.cfg.solver.workers) as executor:
            futures = []
            for i in range(self.cfg.solver.attempts):
                futures.append(executor.submit(self._run_single_attempt, problem, i))
                
            for future in as_completed(futures):
                attempts_data.append(future.result())

        # ç®€å•çš„ç­”æ¡ˆèšåˆ (Extract Final Answer)
        valid_answers = [a['final_answer'] for a in attempts_data if a['final_answer'] is not None]
        
        # ä¼—æ•°æŠ•ç¥¨ (Majority Vote)
        if valid_answers:
            from collections import Counter
            final_consensus = Counter(valid_answers).most_common(1)[0][0]
        else:
            final_consensus = None

        # è®¡ç®—æ‰€æœ‰ attempts çš„æ—¶é—´ç»Ÿè®¡
        attempt_times = [a['time_taken'] for a in attempts_data]
        min_time = min(attempt_times) if attempt_times else 0
        max_time = max(attempt_times) if attempt_times else 0
        avg_time = sum(attempt_times) / len(attempt_times) if attempt_times else 0

        return {
            "id": problem_id,
            "problem": problem,
            "final_answer": final_consensus,
            "attempts": attempts_data,
            "min_attempt_time": min_time,
            "max_attempt_time": max_time,
            "avg_attempt_time": avg_time
        }

    def _run_single_attempt(self, problem: str, attempt_idx: int) -> Dict[str, Any]:
        """
        Core Logic: å•æ¬¡çº¯ CoT æ¨ç†ï¼Œä¸ä½¿ç”¨å·¥å…·
        """
        attempt_start_time = time.time()
        
        messages = [
            {"role": "system", "content": self.cfg.prompts.system_prompt},
            {"role": "user", "content": problem}
        ]
        
        final_answer = None
        total_tokens = 0
        reasoning_content = ""
        response_content = ""
        
        try:
            # å•æ¬¡ LLM è°ƒç”¨ï¼Œä¸ä½¿ç”¨å·¥å…·
            completion_kwargs = {
                'model': self.target_model,
                'messages': messages,
                'temperature': self.cfg.inference.temperature,
                'top_p': self.cfg.inference.top_p,
            }
            if self.cfg.inference.max_tokens is not None:
                completion_kwargs['max_tokens'] = self.cfg.inference.max_tokens

            # è¿½åŠ é¢å¤–æ¨ç†å‚æ•°ï¼ˆä¸ TIRSolver è¡Œä¸ºä¸€è‡´ï¼‰
            if self.cfg.inference.extra:
                completion_kwargs.update(self.cfg.inference.extra)
            
            response = self.client.chat.completions.create(**completion_kwargs)
            message = response.choices[0].message
            
            # ç´¯åŠ  tokens
            if hasattr(response, 'usage') and response.usage:
                total_tokens = response.usage.total_tokens
            
            # è·å–æ¨ç†å†…å®¹ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼Œå¦‚ DeepSeek-R1ï¼‰
            if hasattr(message, 'reasoning_content') and message.reasoning_content:
                reasoning_content = message.reasoning_content
            
            # è·å–å›å¤å†…å®¹
            response_content = message.content or ""
            
            # å°†æ¨¡å‹å›å¤åŠ å…¥å†å²
            messages.append(self._normalize_message(message))
            
            # æå–ç­”æ¡ˆ
            if "\\boxed{" in response_content:
                extracted = self._extract_boxed_content(response_content)
                if extracted:
                    final_answer = extracted
                        
        except Exception as e:
            print(f"Critical Error in CoT attempt {attempt_idx}: {e}")
            messages.append({"role": "system", "content": f"Error: {str(e)}"})

        # æ¸…æ´— messages å¯¹è±¡
        clean_messages = []
        for msg in messages:
            if hasattr(msg, "model_dump"):
                clean_messages.append(msg.model_dump())
            elif hasattr(msg, "to_dict"):
                clean_messages.append(msg.to_dict())
            elif isinstance(msg, dict):
                clean_messages.append(msg)
            else:
                try:
                    clean_messages.append(dict(msg))
                except:
                    clean_messages.append({"role": "unknown", "content": str(msg)})

        return {
            "attempt_id": attempt_idx,
            "final_answer": final_answer,
            "messages": clean_messages,
            "time_taken": time.time() - attempt_start_time,
            "total_tokens": total_tokens
        }

    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """ç®€å•çš„æ­£åˆ™æå– \\boxed{} å†…å®¹"""
        matches = re.findall(r'\\boxed\s*\{(.*?)\}', text)
        if matches:
            return matches[-1]
        return None

    def _normalize_message(self, msg: Any) -> Dict[str, Any]:
        """ç¡®ä¿æ¶ˆæ¯ä¸ºçº¯ dict"""
        if hasattr(msg, "model_dump"):
            return msg.model_dump()
        if hasattr(msg, "to_dict"):
            return msg.to_dict()
        if isinstance(msg, dict):
            return msg
        try:
            return dict(msg)
        except Exception:
            return {"role": "unknown", "content": str(msg)}

    def cleanup(self):
        """CoT Solver ä¸éœ€è¦æ¸…ç†èµ„æºï¼Œä½†ä¿æŒæ¥å£ä¸€è‡´"""
        pass


# ============================================================================
# Harmony TIR Solver - ä½¿ç”¨ Harmony æ¨¡æ¿ + Completion ç«¯å£
# ============================================================================

class HarmonyTemplate:
    """Harmony æ¨¡æ¿å¤„ç†å™¨"""
    
    def get_system_content(self, system_prompt: str, tool_config: "ToolNamespaceConfig") -> "SystemContent":
        return (
            SystemContent.new()
            .with_model_identity(system_prompt)
            .with_reasoning_effort(reasoning_effort=ReasoningEffort.HIGH)
            .with_tools(tool_config)
        )

    def apply_chat_template(
        self,
        system_prompt: str,
        user_prompt: str,
        tool_config: "ToolNamespaceConfig"
    ) -> List["Message"]:
        system_content = self.get_system_content(system_prompt, tool_config)
        system_message = Message.from_role_and_content(Role.SYSTEM, system_content)
        user_message = Message.from_role_and_content(Role.USER, user_prompt)
        return [system_message, user_message]


class HarmonyTool:
    """Harmony å·¥å…·å¤„ç†å™¨ - å¤„ç† Python ä»£ç æ‰§è¡Œ"""
    
    def __init__(self, tool_prompt: str, sandbox: AIMO3Sandbox, jupyter_timeout: float = 30.0):
        self._tool_prompt = tool_prompt
        self._jupyter_session = sandbox
        self._jupyter_timeout = jupyter_timeout
        self._execution_lock = threading.Lock()

    def _ensure_last_print(self, code: str) -> str:
        """ç¡®ä¿æœ€åä¸€è¡Œæœ‰ print è¾“å‡º"""
        lines = code.strip().split('\n')
        if not lines:
            return code
        
        last_line = lines[-1].strip()
        
        # ä¸éœ€è¦å¤„ç†çš„æƒ…å†µ
        if not last_line or 'print' in last_line or 'import' in last_line or last_line.startswith('#'):
            return code
        
        lines[-1] = f'print({last_line})'
        return '\n'.join(lines)

    @property
    def instruction(self) -> str:
        return self._tool_prompt

    @property
    def tool_config(self) -> "ToolNamespaceConfig":
        return ToolNamespaceConfig(
            name='python',
            description=self.instruction,
            tools=[]
        )

    def _make_response(self, output: str, channel: Optional[str] = None) -> "Message":
        """åˆ›å»ºå·¥å…·å“åº”æ¶ˆæ¯"""
        content = TextContent(text=output)
        author = Author(role=Role.TOOL, name='python')
        message = Message(author=author, content=[content]).with_recipient('assistant')
        if channel:
            message = message.with_channel(channel)
        return message

    def process_sync(self, message: "Message") -> List["Message"]:
        """åŒæ­¥å¤„ç†å·¥å…·è°ƒç”¨"""
        raw_script = message.content[0].text
        final_script = self._ensure_last_print(raw_script)
        
        with self._execution_lock:
            try:
                output = self._jupyter_session.execute(final_script)
                if len(output) > 2000:
                    output = output[:2000] + "\n...[Output Truncated]"
            except Exception as e:
                output = f'[ERROR] {str(e)}'
        
        return [self._make_response(output, channel=message.channel)]


class HarmonyTIRSolver:
    """
    ä½¿ç”¨ Harmony æ¨¡æ¿çš„ TIR Solverã€‚
    é€šè¿‡ completion ç«¯å£ä¸ vLLM é€šä¿¡ï¼Œæ‰‹åŠ¨å¤„ç†æ¶ˆæ¯ç¼–ç ã€‚
    ä¸“ä¸º GPT-OSS ç­‰éœ€è¦ Harmony æ¨¡æ¿çš„æ¨¡å‹è®¾è®¡ã€‚
    """
    
    def __init__(self, cfg: "CFG"):
        if not HARMONY_AVAILABLE:
            raise ImportError(
                "openai_harmony æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install openai-harmony"
            )
        
        self.cfg = cfg
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è¿æ¥åœ°å€
        if self.cfg.mode == 'remote':
            print(f"ğŸŒ [Harmony] Connecting to Remote API: {self.cfg.remote.model}")
            api_key = self.cfg.remote.api_key
            base_url = self.cfg.remote.base_url
            self.target_model = self.cfg.remote.model
        else:
            print(f"ğŸ  [Harmony] Connecting to Local vLLM: {self.cfg.local.served_model_name}")
            api_key = "sk-local"
            base_url = f"http://localhost:{self.cfg.local.port}/v1"
            self.target_model = self.cfg.local.served_model_name

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=cfg.solver.timeout_per_problem
        )
        
        # åˆå§‹åŒ– Harmony ç¼–ç å’Œæ¨¡æ¿
        encoding_name = "HARMONY_GPT_OSS"
        self.encoding = load_harmony_encoding(encoding_name)
        self.stop_token_ids = self.encoding.stop_tokens_for_assistant_actions()
        self.template = HarmonyTemplate()
        
        # Harmony ç‰¹å®šé…ç½®
        self.context_tokens = self.cfg.harmony.context_tokens
        self.search_tokens = self.cfg.harmony.search_tokens
        self.buffer_tokens = self.cfg.harmony.buffer_tokens
        self.min_p = self.cfg.harmony.min_p
        self.stream_interval = self.cfg.harmony.stream_interval
        
        # åˆå§‹åŒ– Sandbox Pool
        self.sandbox_pool = queue.Queue()
        self._init_sandboxes()

    def _init_sandboxes(self):
        """åˆå§‹åŒ–æ²™ç®±æ± """
        print(f"ğŸ”§ [Harmony] Initializing {self.cfg.solver.workers} sandboxes...")
        with ThreadPoolExecutor(max_workers=self.cfg.solver.workers) as exe:
            futures = [exe.submit(AIMO3Sandbox, timeout=30) for _ in range(self.cfg.solver.workers)]
            for f in as_completed(futures):
                self.sandbox_pool.put(f.result())
        print("âœ… [Harmony] Sandboxes ready.")

    def solve(self, problem: str, problem_id: str) -> Dict[str, Any]:
        """
        Orchestrator: å¹¶å‘æ‰§è¡Œå¤šæ¬¡å°è¯• (Maj@k)
        """
        start_time = time.time()
        attempts_data = []
        
        # å¹¶è¡Œæ‰§è¡Œ k æ¬¡é‡‡æ ·
        with ThreadPoolExecutor(max_workers=self.cfg.solver.workers) as executor:
            futures = []
            for i in range(self.cfg.solver.attempts):
                futures.append(executor.submit(self._run_single_attempt, problem, i))
                
            for future in as_completed(futures):
                attempts_data.append(future.result())

        # ç­”æ¡ˆèšåˆ
        valid_answers = [a['final_answer'] for a in attempts_data if a['final_answer'] is not None]
        
        # ä¼—æ•°æŠ•ç¥¨ (Majority Vote)
        if valid_answers:
            final_consensus = Counter(valid_answers).most_common(1)[0][0]
        else:
            final_consensus = None

        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        attempt_times = [a['time_taken'] for a in attempts_data]
        min_time = min(attempt_times) if attempt_times else 0
        max_time = max(attempt_times) if attempt_times else 0
        avg_time = sum(attempt_times) / len(attempt_times) if attempt_times else 0

        return {
            "id": problem_id,
            "problem": problem,
            "final_answer": final_consensus,
            "attempts": attempts_data,
            "min_attempt_time": min_time,
            "max_attempt_time": max_time,
            "avg_attempt_time": avg_time
        }

    def _run_single_attempt(self, problem: str, attempt_idx: int) -> Dict[str, Any]:
        """
        Core Logic: å•æ¬¡ Harmony TIR å¾ªç¯
        ä½¿ç”¨ completion ç«¯å£å’Œæµå¼å¤„ç†
        """
        attempt_start_time = time.time()
        sandbox = self.sandbox_pool.get()
        
        # åˆå§‹åŒ–å·¥å…·
        local_tool = HarmonyTool(
            tool_prompt=self.cfg.prompts.tool_prompt,
            sandbox=sandbox,
            jupyter_timeout=30.0
        )
        
        final_answer = None
        python_calls = 0
        python_errors = 0
        total_tokens = 0
        
        # è®¡ç®— attempt çš„ç§å­
        attempt_seed = int(self.cfg.solver.seed + attempt_idx) ** 2
        
        try:
            # æ„å»ºåˆå§‹æ¶ˆæ¯
            messages = self.template.apply_chat_template(
                self.cfg.prompts.system_prompt,
                problem,
                local_tool.tool_config
            )
            conversation = Conversation.from_messages(messages)
            
            max_turns = self.cfg.inference.max_turns
            
            for turn in range(max_turns):
                # å°†ä¼šè¯æ¸²æŸ“ä¸º token IDs
                prompt_ids = self.encoding.render_conversation_for_completion(
                    conversation, Role.ASSISTANT
                )
                max_tokens = self.context_tokens - len(prompt_ids)
                
                if max_tokens < self.buffer_tokens:
                    print(f"âš ï¸ [Harmony] Attempt {attempt_idx}: Context exhausted")
                    break
                
                # è°ƒç”¨ completion ç«¯å£ï¼ˆæµå¼ï¼‰
                try:
                    stream = self.client.completions.create(
                        model=self.target_model,
                        temperature=self.cfg.inference.temperature,
                        max_tokens=max_tokens,
                        prompt=prompt_ids,
                        seed=attempt_seed,
                        stream=True,
                        extra_body={
                            'min_p': self.min_p,
                            'stop_token_ids': self.stop_token_ids,
                            'return_token_ids': True
                        }
                    )
                except Exception as e:
                    print(f"âš ï¸ [Harmony] Stream creation failed: {e}")
                    break
                
                # å¤„ç†æµå¼å“åº”
                token_buffer = []
                text_chunks = []
                
                try:
                    for chunk in stream:
                        choice = chunk.choices[0]
                        new_tokens = getattr(choice, 'token_ids', None)
                        new_text = choice.text
                        
                        if new_tokens:
                            token_buffer.extend(new_tokens)
                            total_tokens += len(new_tokens)
                            text_chunks.append(new_text)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ boxed ç­”æ¡ˆ
                        if '}' in new_text:
                            search_text = ''.join(text_chunks[-self.search_tokens:])
                            answer = self._scan_for_answer(search_text)
                            if answer is not None:
                                final_answer = answer
                                break
                finally:
                    stream.close()
                
                if final_answer is not None:
                    break
                
                if not token_buffer:
                    break
                
                # è§£ææ–°æ¶ˆæ¯
                new_messages = self.encoding.parse_messages_from_completion_tokens(
                    token_buffer, Role.ASSISTANT
                )
                conversation.messages.extend(new_messages)
                last_message = new_messages[-1]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆç­”æ¡ˆ
                if last_message.channel == 'final':
                    answer_text = last_message.content[0].text
                    final_answer = self._scan_for_answer(answer_text)
                    break
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
                if last_message.recipient == 'python':
                    python_calls += 1
                    tool_responses = local_tool.process_sync(last_message)
                    
                    response_text = tool_responses[0].content[0].text
                    if response_text.startswith('[ERROR]') or 'Traceback' in response_text or 'Error:' in response_text:
                        python_errors += 1
                    
                    conversation.messages.extend(tool_responses)
                    
        except Exception as e:
            print(f"Critical Error in Harmony attempt {attempt_idx}: {e}")
            python_errors += 1
        finally:
            sandbox.reset()
            self.sandbox_pool.put(sandbox)

        # æ¸…æ´—æ¶ˆæ¯ç”¨äºè¿”å›
        clean_messages = self._clean_conversation(conversation)

        return {
            "attempt_id": attempt_idx,
            "final_answer": final_answer,
            "messages": clean_messages,
            "time_taken": time.time() - attempt_start_time,
            "python_calls": python_calls,
            "python_errors": python_errors,
            "total_tokens": total_tokens
        }

    def _scan_for_answer(self, text: str) -> Optional[int]:
        """æ‰«ææ–‡æœ¬ä¸­çš„ \\boxed{} ç­”æ¡ˆ"""
        pattern = r'\\boxed\s*\{\s*([0-9,]+)\s*\}'
        matches = re.findall(pattern, text)
        
        if matches:
            try:
                clean_value = matches[-1].replace(',', '')
                value = int(clean_value)
                if 0 <= value <= 99999:
                    return value
            except ValueError:
                pass
        return None

    def _clean_conversation(self, conversation: "Conversation") -> List[Dict[str, Any]]:
        """å°† Conversation å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸åˆ—è¡¨"""
        clean_messages = []
        for msg in conversation.messages:
            try:
                if hasattr(msg, 'model_dump'):
                    clean_messages.append(msg.model_dump())
                elif hasattr(msg, 'to_dict'):
                    clean_messages.append(msg.to_dict())
                else:
                    # æ‰‹åŠ¨æ„å»ºå­—å…¸
                    content_text = ""
                    if hasattr(msg, 'content') and msg.content:
                        if hasattr(msg.content[0], 'text'):
                            content_text = msg.content[0].text
                        else:
                            content_text = str(msg.content)
                    
                    role = "unknown"
                    if hasattr(msg, 'author') and hasattr(msg.author, 'role'):
                        role = str(msg.author.role.value) if hasattr(msg.author.role, 'value') else str(msg.author.role)
                    
                    clean_messages.append({
                        "role": role,
                        "content": content_text
                    })
            except Exception:
                clean_messages.append({"role": "unknown", "content": str(msg)})
        return clean_messages

    def cleanup(self):
        """å…³é—­æ²™ç®±"""
        while not self.sandbox_pool.empty():
            try:
                sb = self.sandbox_pool.get_nowait()
                sb.close()
            except:
                pass