import json
import time
import re
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional, TYPE_CHECKING

from openai import OpenAI
from aimo3_eval.engine.sandbox import AIMO3Sandbox

# ä¸ºäº†ç±»å‹æç¤ºï¼Œé˜²æ­¢è¿è¡Œæ—¶å¾ªç¯å¼•ç”¨
if TYPE_CHECKING:
    from aimo3_eval.config import CFG

# --- å®šä¹‰å·¥å…· Schema ---
PYTHON_TOOL = [
    {
        "type": "function",
        "function": {
            "name": "python_interpreter",
            "description": "Execute Python code in a stateful Jupyter notebook environment. Use print() to see outputs.",
            "parameters": {
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "description": "The python code to execute. Variables are preserved between calls."
                    }
                },
                "required": ["code"]
            }
        }
    }
]

class TIRSolver:
    def __init__(self, cfg: "CFG"):
        self.cfg = cfg
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è¿æ¥åœ°å€
        if self.cfg.mode == 'remote':
            print(f"ğŸŒ Connecting to Remote API: {self.cfg.remote_model_name}")
            api_key = self.cfg.remote_api_key
            base_url = self.cfg.remote_base_url
            self.target_model = self.cfg.remote_model_name
        else:
            print(f"ğŸ  Connecting to Local vLLM: {self.cfg.served_model_name}")
            api_key = "sk-local"
            base_url = f"http://localhost:{self.cfg.port}/v1"
            self.target_model = self.cfg.served_model_name

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=cfg.timeout_per_problem
        )
        
        # åˆå§‹åŒ– Sandbox Pool
        self.sandbox_pool = queue.Queue()
        self._init_sandboxes()

    def _init_sandboxes(self):
        print(f"ğŸ”§ Initializing {self.cfg.workers} sandboxes...")
        with ThreadPoolExecutor(max_workers=self.cfg.workers) as exe:
            # ä¼ é€’ timeout å‚æ•°
            futures = [exe.submit(AIMO3Sandbox, timeout=30) for _ in range(self.cfg.workers)]
            for f in as_completed(futures):
                self.sandbox_pool.put(f.result())
        print("âœ… Sandboxes ready.")

    def solve(self, problem: str, problem_id: str) -> Dict[str, Any]:
        """
        Orchestrator: å¹¶å‘æ‰§è¡Œå¤šæ¬¡å°è¯• (Maj@k)
        """
        start_time = time.time()
        attempts_data = []
        
        # å¹¶è¡Œæ‰§è¡Œ k æ¬¡é‡‡æ ·
        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:
            futures = []
            for i in range(self.cfg.attempts):
                futures.append(executor.submit(self._run_single_attempt, problem, i))
                
            for future in as_completed(futures):
                attempts_data.append(future.result())

        # ç®€å•çš„ç­”æ¡ˆèšåˆ (Extract Final Answer)
        valid_answers = [a['final_answer'] for a in attempts_data if a['final_answer'] is not None]
        
        # ä¼—æ•°æŠ•ç¥¨ (Majority Vote)
        if valid_answers:
            from collections import Counter
            final_consensus = Counter(valid_answers).most_common(1)[0][0]
        else:
            final_consensus = None

        # è®¡ç®—æ‰€æœ‰ attempts çš„æ—¶é—´ç»Ÿè®¡
        attempt_times = [a['time_taken'] for a in attempts_data]
        min_time = min(attempt_times) if attempt_times else 0
        max_time = max(attempt_times) if attempt_times else 0
        avg_time = sum(attempt_times) / len(attempt_times) if attempt_times else 0

        return {
            "id": problem_id,
            "problem": problem,
            "final_answer": final_consensus,
            "attempts": attempts_data,  # ä¿å­˜å®Œæ•´è½¨è¿¹ä¾›å¤ç›˜
            "min_attempt_time": min_time,  # æœ€çŸ­ attempt æ—¶é—´
            "max_attempt_time": max_time,  # æœ€é•¿ attempt æ—¶é—´
            "avg_attempt_time": avg_time   # å¹³å‡ attempt æ—¶é—´
        }

    def _run_single_attempt(self, problem: str, attempt_idx: int) -> Dict[str, Any]:
        """
        Core Logic: å•æ¬¡ TIR (Tool-Integrated Reasoning) å¾ªç¯
        """
        attempt_start_time = time.time()  # è®°å½• attempt å¼€å§‹æ—¶é—´
        sandbox = self.sandbox_pool.get() # ä»æ± ä¸­è·å–æ²™ç®±
        
        messages = [
            {"role": "system", "content": self.cfg.system_prompt},
            {"role": "user", "content": problem}
        ]
        
        final_answer = None
        turn_count = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        python_calls = 0
        python_errors = 0
        total_tokens = 0
        
        try:
            # --- The Main Loop ---
            # ä½¿ç”¨ getattr è·å– max_turnsï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™é»˜è®¤ 7
            max_turns = getattr(self.cfg, 'max_turns', 7)
            
            while turn_count < max_turns:
                turn_count += 1
                
                # 1. è°ƒç”¨ LLM
                try:
                    completion_kwargs = {
                        'model': self.target_model,
                        'messages': messages,
                        'tools': PYTHON_TOOL,
                        'temperature': self.cfg.temperature,
                    }
                    if self.cfg.max_tokens is not None:
                        completion_kwargs['max_tokens'] = self.cfg.max_tokens
                    
                    response = self.client.chat.completions.create(**completion_kwargs)
                    message = response.choices[0].message
                    
                    # ç´¯åŠ  tokens
                    if hasattr(response, 'usage') and response.usage:
                        total_tokens += response.usage.total_tokens
                        
                except Exception as e:
                    messages.append({"role": "system", "content": f"Error: {str(e)}"})
                    break

                # 2. å°†æ¨¡å‹çš„å›å¤åŠ å…¥å†å²ï¼Œè½¬ä¸ºçº¯å­—å…¸ï¼Œé¿å…ä¸‹æ¬¡è¯·æ±‚æºå¸¦ SDK å¯¹è±¡
                messages.append(self._normalize_message(message))

                # 3. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ (Tool Calls)
                if message.tool_calls:
                    for tool_call in message.tool_calls:
                        if tool_call.function.name == "python_interpreter":
                            python_calls += 1  # è®°å½• Python è°ƒç”¨æ¬¡æ•°
                            
                            # A. è§£æä»£ç 
                            try:
                                arguments = json.loads(tool_call.function.arguments)
                                code = arguments.get("code", "")
                            except json.JSONDecodeError:
                                code = ""
                                output = "Error: Invalid JSON format in tool arguments."
                                python_errors += 1

                            # B. æ²™ç®±æ‰§è¡Œ
                            if code:
                                try:
                                    output = sandbox.execute(code)
                                    if len(output) > 2000:
                                        output = output[:2000] + "\n...[Output Truncated]"
                                except Exception as e:
                                    output = f"Execution Error: {str(e)}"
                                    python_errors += 1  # è®°å½•æ‰§è¡Œé”™è¯¯
                            else:
                                output = "Error: No code provided."
                                python_errors += 1

                            # C. å°†ç»“æœè¿½åŠ å›æ¶ˆæ¯åˆ—è¡¨ (Role: Tool)
                            messages.append({
                                "role": "tool",
                                "tool_call_id": tool_call.id,
                                "name": tool_call.function.name,
                                "content": output
                            })
                
                # 4. å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç»å¾—å‡ºç­”æ¡ˆ
                else:
                    content = message.content or ""
                    if "\\boxed{" in content:
                        extracted = self._extract_boxed_content(content)
                        if extracted:
                            final_answer = extracted
                            break
                    
        except Exception as e:
            print(f"Critical Error in attempt {attempt_idx}: {e}")
        finally:
            sandbox.reset()
            self.sandbox_pool.put(sandbox)

        # === å…³é”®ä¿®å¤ï¼šæ¸…æ´— messages å¯¹è±¡ï¼Œé˜²æ­¢ Polars æŠ¥é”™ ===
        clean_messages = []
        for msg in messages:
            if hasattr(msg, "model_dump"):
                # OpenAI v1.x+ (Pydantic V2)
                clean_messages.append(msg.model_dump())
            elif hasattr(msg, "to_dict"):
                # æ—§ç‰ˆæœ¬æˆ–æŸäº›å…¼å®¹åº“
                clean_messages.append(msg.to_dict())
            elif isinstance(msg, dict):
                clean_messages.append(msg)
            else:
                # å…œåº•è½¬æ¢
                try:
                    clean_messages.append(dict(msg))
                except:
                    # æœ€åçš„é˜²çº¿ï¼šè½¬å­—ç¬¦ä¸²
                    clean_messages.append({"role": "unknown", "content": str(msg)})

        return {
            "attempt_id": attempt_idx,
            "final_answer": final_answer,
            "messages": clean_messages,  # <--- è¿”å›æ¸…æ´—åçš„å­—å…¸åˆ—è¡¨
            "time_taken": time.time() - attempt_start_time,  # è®°å½•è¯¥ attempt çš„è€—æ—¶
            "python_calls": python_calls,  # Python è°ƒç”¨æ¬¡æ•°
            "python_errors": python_errors,  # Python é”™è¯¯æ¬¡æ•°
            "total_tokens": total_tokens  # æ€» token ä½¿ç”¨é‡
        }

    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """
        ç®€å•çš„æ­£åˆ™æå–ï¼ŒPhase 2 å·²æœ‰æ›´å¼ºçš„ Extractorï¼Œè¿™é‡Œåªæ˜¯ Loop å†…çš„å¿«é€Ÿæ£€æŸ¥
        """
        matches = re.findall(r'\\boxed\s*\{(.*?)\}', text)
        if matches:
            return matches[-1]
        return None

    def _normalize_message(self, msg: Any) -> Dict[str, Any]:
        """ç¡®ä¿æ¶ˆæ¯ä¸ºçº¯ dictï¼Œé¿å… ChatCompletionMessage ç­‰ SDK å¯¹è±¡åœ¨ä¸‹ä¸€è½®è°ƒç”¨å‡ºé”™"""
        if hasattr(msg, "model_dump"):
            return msg.model_dump()
        if hasattr(msg, "to_dict"):
            return msg.to_dict()
        if isinstance(msg, dict):
            return msg
        try:
            return dict(msg)
        except Exception:
            return {"role": "unknown", "content": str(msg)}

    def cleanup(self):
        """Shut down sandboxes"""
        while not self.sandbox_pool.empty():
            try:
                sb = self.sandbox_pool.get_nowait()
                sb.close()
            except:
                pass


class CoTSolver:
    """
    çº¯ Chain-of-Thought Solverï¼Œä¸ä½¿ç”¨ä»»ä½•å·¥å…·ã€‚
    æ¨¡å‹ç›´æ¥é€šè¿‡æ¨ç†å¾—å‡ºç­”æ¡ˆã€‚
    """
    
    def __init__(self, cfg: "CFG"):
        self.cfg = cfg
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©è¿æ¥åœ°å€
        if self.cfg.mode == 'remote':
            print(f"ğŸŒ [CoT] Connecting to Remote API: {self.cfg.remote_model_name}")
            api_key = self.cfg.remote_api_key
            base_url = self.cfg.remote_base_url
            self.target_model = self.cfg.remote_model_name
        else:
            print(f"ğŸ  [CoT] Connecting to Local vLLM: {self.cfg.served_model_name}")
            api_key = "sk-local"
            base_url = f"http://localhost:{self.cfg.port}/v1"
            self.target_model = self.cfg.served_model_name

        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self.client = OpenAI(
            base_url=base_url,
            api_key=api_key,
            timeout=cfg.timeout_per_problem
        )

    def solve(self, problem: str, problem_id: str) -> Dict[str, Any]:
        """
        Orchestrator: å¹¶å‘æ‰§è¡Œå¤šæ¬¡å°è¯• (Maj@k)
        """
        start_time = time.time()
        attempts_data = []
        
        # å¹¶è¡Œæ‰§è¡Œ k æ¬¡é‡‡æ ·
        with ThreadPoolExecutor(max_workers=self.cfg.workers) as executor:
            futures = []
            for i in range(self.cfg.attempts):
                futures.append(executor.submit(self._run_single_attempt, problem, i))
                
            for future in as_completed(futures):
                attempts_data.append(future.result())

        # ç®€å•çš„ç­”æ¡ˆèšåˆ (Extract Final Answer)
        valid_answers = [a['final_answer'] for a in attempts_data if a['final_answer'] is not None]
        
        # ä¼—æ•°æŠ•ç¥¨ (Majority Vote)
        if valid_answers:
            from collections import Counter
            final_consensus = Counter(valid_answers).most_common(1)[0][0]
        else:
            final_consensus = None

        # è®¡ç®—æ‰€æœ‰ attempts çš„æ—¶é—´ç»Ÿè®¡
        attempt_times = [a['time_taken'] for a in attempts_data]
        min_time = min(attempt_times) if attempt_times else 0
        max_time = max(attempt_times) if attempt_times else 0
        avg_time = sum(attempt_times) / len(attempt_times) if attempt_times else 0

        return {
            "id": problem_id,
            "problem": problem,
            "final_answer": final_consensus,
            "attempts": attempts_data,
            "min_attempt_time": min_time,
            "max_attempt_time": max_time,
            "avg_attempt_time": avg_time
        }

    def _run_single_attempt(self, problem: str, attempt_idx: int) -> Dict[str, Any]:
        """
        Core Logic: å•æ¬¡çº¯ CoT æ¨ç†ï¼Œä¸ä½¿ç”¨å·¥å…·
        """
        attempt_start_time = time.time()
        
        messages = [
            {"role": "system", "content": self.cfg.system_prompt},
            {"role": "user", "content": problem}
        ]
        
        final_answer = None
        total_tokens = 0
        reasoning_content = ""
        response_content = ""
        
        try:
            # å•æ¬¡ LLM è°ƒç”¨ï¼Œä¸ä½¿ç”¨å·¥å…·
            completion_kwargs = {
                'model': self.target_model,
                'messages': messages,
                'temperature': self.cfg.temperature,
            }
            if self.cfg.max_tokens is not None:
                completion_kwargs['max_tokens'] = self.cfg.max_tokens
            
            response = self.client.chat.completions.create(**completion_kwargs)
            message = response.choices[0].message
            
            # ç´¯åŠ  tokens
            if hasattr(response, 'usage') and response.usage:
                total_tokens = response.usage.total_tokens
            
            # è·å–æ¨ç†å†…å®¹ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼Œå¦‚ DeepSeek-R1ï¼‰
            if hasattr(message, 'reasoning_content') and message.reasoning_content:
                reasoning_content = message.reasoning_content
            
            # è·å–å›å¤å†…å®¹
            response_content = message.content or ""
            
            # å°†æ¨¡å‹å›å¤åŠ å…¥å†å²
            messages.append(self._normalize_message(message))
            
            # æå–ç­”æ¡ˆ
            if "\\boxed{" in response_content:
                extracted = self._extract_boxed_content(response_content)
                if extracted:
                    final_answer = extracted
                        
        except Exception as e:
            print(f"Critical Error in CoT attempt {attempt_idx}: {e}")
            messages.append({"role": "system", "content": f"Error: {str(e)}"})

        # æ¸…æ´— messages å¯¹è±¡
        clean_messages = []
        for msg in messages:
            if hasattr(msg, "model_dump"):
                clean_messages.append(msg.model_dump())
            elif hasattr(msg, "to_dict"):
                clean_messages.append(msg.to_dict())
            elif isinstance(msg, dict):
                clean_messages.append(msg)
            else:
                try:
                    clean_messages.append(dict(msg))
                except:
                    clean_messages.append({"role": "unknown", "content": str(msg)})

        return {
            "attempt_id": attempt_idx,
            "final_answer": final_answer,
            "messages": clean_messages,
            "time_taken": time.time() - attempt_start_time,
            "total_tokens": total_tokens
        }

    def _extract_boxed_content(self, text: str) -> Optional[str]:
        """ç®€å•çš„æ­£åˆ™æå– \\boxed{} å†…å®¹"""
        matches = re.findall(r'\\boxed\s*\{(.*?)\}', text)
        if matches:
            return matches[-1]
        return None

    def _normalize_message(self, msg: Any) -> Dict[str, Any]:
        """ç¡®ä¿æ¶ˆæ¯ä¸ºçº¯ dict"""
        if hasattr(msg, "model_dump"):
            return msg.model_dump()
        if hasattr(msg, "to_dict"):
            return msg.to_dict()
        if isinstance(msg, dict):
            return msg
        try:
            return dict(msg)
        except Exception:
            return {"role": "unknown", "content": str(msg)}

    def cleanup(self):
        """CoT Solver ä¸éœ€è¦æ¸…ç†èµ„æºï¼Œä½†ä¿æŒæ¥å£ä¸€è‡´"""
        pass